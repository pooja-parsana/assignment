{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "204085a0-c557-45da-a912-515f60367106",
   "metadata": {},
   "source": [
    "Q1. What is the curse of dimensionality reduction and why is it important in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1225940b-2cb3-4639-844c-cace878a5173",
   "metadata": {},
   "source": [
    "The \"curse of dimensionality\" refers to the various challenges and problems that arise when working with high-dimensional data in machine learning and other fields. It primarily affects algorithms that rely on distance or similarity measures, such as clustering, nearest neighbor search, and certain types of optimization. The curse of dimensionality becomes particularly pronounced as the number of dimensions in the data increases.\n",
    "\n",
    "Here are some key aspects of the curse of dimensionality and why it's important in machine learning:\n",
    "\n",
    "1. **Sparse Data Distribution**: As the number of dimensions increases, the available data becomes sparser in the high-dimensional space. This means that the data points are more spread out and farther away from each other, which can lead to difficulties in finding meaningful patterns or relationships.\n",
    "\n",
    "2. **Increased Computational Complexity**: Many algorithms' time and memory requirements grow exponentially with the number of dimensions. This leads to significantly increased computational costs, making the algorithms slow and resource-intensive.\n",
    "\n",
    "3. **Overfitting**: With high-dimensional data, there's a higher risk of overfitting. Models can fit noise and random variations in the data rather than genuine patterns, leading to poor generalization performance on new, unseen data.\n",
    "\n",
    "4. **Curse of High-Dimensional Visualization**: Humans are limited in their ability to visualize data beyond three dimensions. As the dimensionality increases, it becomes increasingly difficult to visualize and understand the data, which makes it challenging to perform exploratory data analysis and understand the underlying structures.\n",
    "\n",
    "5. **Distance Metric Distortion**: In high-dimensional spaces, the concept of distance becomes distorted. Points that are far apart in high dimensions might appear close when measured by some distance metrics. This distortion affects algorithms that rely on distances or similarities, potentially leading to incorrect results.\n",
    "\n",
    "6. **Data Sparsity and Nearest Neighbors**: In high dimensions, data points tend to become equidistant from each other, affecting the concept of \"nearest neighbors.\" This can make nearest neighbor-based methods less effective, as neighboring points might not be truly representative or meaningful.\n",
    "\n",
    "7. **Feature Selection and Curse**: Including irrelevant or redundant features in high-dimensional data can lead to decreased model performance. Feature selection and dimensionality reduction techniques become crucial to mitigate the impact of irrelevant features.\n",
    "\n",
    "Addressing the curse of dimensionality is important in machine learning because it directly impacts the performance and efficiency of algorithms. Techniques for dimensionality reduction, such as Principal Component Analysis (PCA), t-SNE, and autoencoders, are used to reduce the number of dimensions while retaining as much useful information as possible. These techniques help in visualizing data, improving algorithm efficiency, mitigating overfitting, and enhancing the interpretability of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a2477-262c-4e1f-bc99-7cda94dd4106",
   "metadata": {},
   "source": [
    "Q2. How does the curse of dimensionality impact the performance of machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb89d4d-a1d3-474e-8bef-abcf2d0c3fae",
   "metadata": {},
   "source": [
    "The curse of dimensionality can have significant negative effects on the performance of machine learning algorithms. Here's how it impacts various aspects of algorithm performance:\n",
    "\n",
    "1. **Increased Computational Complexity**: As the number of dimensions increases, the computational resources required to process and analyze the data grow exponentially. Algorithms that involve distance calculations, such as k-nearest neighbors or clustering algorithms, become much slower and resource-intensive in high-dimensional spaces. This can lead to impractical and slow training and inference times.\n",
    "\n",
    "2. **Overfitting**: High-dimensional data provides more room for models to fit noise and random variations. This can lead to overfitting, where a model learns the noise in the data rather than the underlying patterns. As a result, the model's performance might degrade on new, unseen data, and its ability to generalize diminishes.\n",
    "\n",
    "3. **Sparse Data Distribution**: In high-dimensional spaces, data points become increasingly sparse. This sparsity can make it difficult for algorithms to find meaningful patterns or relationships within the data. Important regions of the data space might not have sufficient data points to accurately represent the true distribution.\n",
    "\n",
    "4. **Curse of Visualization**: Humans can easily visualize data in 2D or 3D, but high-dimensional data is difficult to visualize. This makes it challenging to perform exploratory data analysis, understand the data's underlying structure, and identify outliers or anomalies.\n",
    "\n",
    "5. **Distance Metric Distortion**: Many algorithms rely on calculating distances or similarities between data points. In high-dimensional spaces, the concept of distance becomes distorted, as points that are far apart in one dimension might appear close in others. This distortion can lead to incorrect clustering, classification, or regression results.\n",
    "\n",
    "6. **Diminished Nearest Neighbor Effectiveness**: Nearest neighbor-based algorithms, like k-nearest neighbors, become less effective in high dimensions due to equidistant data points. The notion of \"nearest neighbors\" becomes less meaningful, as points are roughly equidistant from each other, affecting the algorithm's ability to capture the local structure of the data.\n",
    "\n",
    "7. **Feature Selection and Irrelevant Features**: Including irrelevant or redundant features in high-dimensional data can lead to decreased model performance. These features can introduce noise and increase the complexity of the problem. Feature selection techniques become essential to identify and retain only the most informative features.\n",
    "\n",
    "8. **Model Complexity and Generalization**: High-dimensional data can lead to complex models that might not generalize well to new data. More parameters are required to fit the data, increasing the risk of overfitting and making the model less interpretable.\n",
    "\n",
    "To mitigate the negative impact of the curse of dimensionality, various techniques are employed in machine learning:\n",
    "\n",
    "- **Dimensionality Reduction**: Techniques like Principal Component Analysis (PCA), t-SNE, and autoencoders reduce the number of dimensions while retaining important information. This can help improve algorithm efficiency and reduce overfitting.\n",
    "\n",
    "- **Feature Selection**: Careful selection of relevant features helps in reducing the dimensionality of the data by removing less informative ones.\n",
    "\n",
    "- **Regularization**: Applying regularization techniques can help control model complexity and prevent overfitting.\n",
    "\n",
    "- **Algorithm Selection**: Choosing algorithms that are less affected by high dimensionality, or those specifically designed to handle it, can lead to better results. For example, tree-based algorithms might be less sensitive to high-dimensional data compared to distance-based algorithms.\n",
    "\n",
    "In summary, the curse of dimensionality highlights the challenges that arise when dealing with high-dimensional data in machine learning, impacting algorithm performance, computational efficiency, and generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bde592a-7bb6-4dc7-ae70-79c5a18d7a67",
   "metadata": {},
   "source": [
    "Q3. What are some of the consequences of the curse of dimensionality in machine learning, and how do\n",
    "they impact model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b1a70-113c-4762-86c7-28fc75d7edb3",
   "metadata": {},
   "source": [
    "The curse of dimensionality can have several consequences in machine learning, and these consequences can significantly impact the performance of models. Here are some of the key consequences and their effects on model performance:\n",
    "\n",
    "1. **Increased Data Sparsity**: As the number of dimensions increases, data points become more spread out in the high-dimensional space. This increased sparsity means that there are fewer data points in the vicinity of any given point, making it harder for algorithms to capture meaningful patterns. This can lead to decreased accuracy and predictive power of models.\n",
    "\n",
    "2. **Overfitting**: With high-dimensional data, models have a greater capacity to fit noise rather than actual patterns. This results in overfitting, where a model performs well on the training data but poorly on new, unseen data. The model fails to generalize because it's learning the noise and variations specific to the training set.\n",
    "\n",
    "3. **Computational Complexity**: Algorithms that involve calculations of distances or pairwise interactions between data points become computationally expensive in high-dimensional spaces. The increased computational complexity can lead to longer training times and resource constraints, making certain algorithms impractical for high-dimensional data.\n",
    "\n",
    "4. **Reduced Model Interpretability**: High-dimensional models can become overly complex, making them difficult to interpret and understand. The relationships between features and outcomes become convoluted, and it's challenging to explain how specific features contribute to predictions.\n",
    "\n",
    "5. **Degraded Nearest Neighbor-Based Algorithms**: Nearest neighbor-based algorithms, which rely on finding similar data points, become less effective in high dimensions. Equidistant data points and distorted distances can lead to incorrect neighbor assignments, reducing the accuracy of these algorithms.\n",
    "\n",
    "6. **Increased Feature Irrelevance**: In high-dimensional spaces, there's a higher chance of including irrelevant or redundant features. These features introduce noise and can negatively impact model performance, making it harder for the model to discern relevant patterns from noise.\n",
    "\n",
    "7. **Diminished Clustering Accuracy**: Clustering algorithms might struggle to identify meaningful clusters in high-dimensional data due to the increased sparsity and distortion of distances. Clusters can become diffuse and less distinct, leading to decreased accuracy in grouping similar data points.\n",
    "\n",
    "8. **Difficulty in Visualizing Data**: Humans are limited in their ability to visualize data beyond three dimensions. With high-dimensional data, it becomes nearly impossible to visualize the data distribution and relationships. This hampers exploratory data analysis and a deeper understanding of the data.\n",
    "\n",
    "9. **Increased Model Complexity**: High-dimensional models often require more parameters to fit the data adequately. This increased complexity can make models harder to optimize, fine-tune, and maintain, leading to potential performance bottlenecks.\n",
    "\n",
    "To mitigate these consequences and improve model performance in the face of the curse of dimensionality, various techniques are employed:\n",
    "\n",
    "- **Dimensionality Reduction**: Techniques like PCA, t-SNE, and LDA are used to reduce the number of dimensions while preserving important information.\n",
    "  \n",
    "- **Feature Selection**: Identifying and selecting relevant features help reduce noise and improve model accuracy and interpretability.\n",
    "  \n",
    "- **Regularization**: Applying regularization techniques can help prevent overfitting by penalizing complex models.\n",
    "\n",
    "- **Algorithm Selection**: Choosing algorithms that are less affected by high dimensionality or using ensemble methods can lead to better results.\n",
    "\n",
    "- **Domain Knowledge**: Leveraging domain knowledge to guide feature selection and model building can help improve model performance.\n",
    "\n",
    "In summary, the curse of dimensionality can lead to reduced model performance, increased computational demands, and challenges in model interpretation. Effective dimensionality reduction and feature selection strategies, along with careful algorithm choice, are crucial to mitigating these consequences and building accurate and efficient models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb253b6a-3df4-4ff6-a812-9e9f55708100",
   "metadata": {},
   "source": [
    "Q4. Can you explain the concept of feature selection and how it can help with dimensionality reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9eefbc-d5a8-4941-9a9d-b5e7d60b751c",
   "metadata": {},
   "source": [
    "Feature selection is the process of selecting a subset of relevant features or variables from a larger set of available features in a dataset. The goal of feature selection is to improve the performance of machine learning models by reducing the complexity of the data, enhancing model interpretability, and preventing overfitting. It is a key technique for addressing the curse of dimensionality and improving the efficiency and accuracy of models.\n",
    "\n",
    "Here's how feature selection works and how it helps with dimensionality reduction:\n",
    "\n",
    "1. **Motivation**: In many real-world datasets, not all features are equally important for making accurate predictions. Some features might be redundant, irrelevant, or even noisy, and including them can lead to suboptimal model performance. Feature selection aims to identify and retain only the most informative features that contribute significantly to the target variable.\n",
    "\n",
    "2. **Benefits of Dimensionality Reduction**: One of the primary benefits of feature selection is dimensionality reduction. By selecting a subset of features, the overall dimensionality of the dataset is reduced. This can lead to several advantages, including:\n",
    "\n",
    "   - **Reduced Computational Complexity**: With fewer features, the computational resources required for training and inference decrease, making models faster and more efficient.\n",
    "   \n",
    "   - **Improved Model Performance**: Removing irrelevant or noisy features can lead to improved model accuracy and generalization, as the model focuses on the most important information.\n",
    "   \n",
    "   - **Less Risk of Overfitting**: Dimensionality reduction through feature selection reduces the risk of overfitting by reducing the complexity of the model. Models are less likely to memorize noise in the data.\n",
    "   \n",
    "   - **Enhanced Interpretability**: Models with fewer features are easier to understand and interpret. The relationships between features and outcomes become clearer.\n",
    "\n",
    "3. **Methods of Feature Selection**:\n",
    "\n",
    "   - **Filter Methods**: These methods use statistical measures to rank features based on their relevance to the target variable. Features are then selected based on their rankings. Examples include correlation-based methods and statistical tests like chi-squared.\n",
    "   \n",
    "   - **Wrapper Methods**: These methods involve using a specific machine learning algorithm to evaluate subsets of features. Different subsets are evaluated, and the best subset is chosen based on model performance. Recursive Feature Elimination (RFE) is an example of a wrapper method.\n",
    "   \n",
    "   - **Embedded Methods**: These methods incorporate feature selection into the model training process itself. Regularized models like Lasso (L1 regularization) naturally perform feature selection by pushing the coefficients of irrelevant features towards zero.\n",
    "   \n",
    "   - **Hybrid Methods**: These methods combine aspects of multiple approaches, often incorporating a filter or wrapper method into the training process of an embedded method.\n",
    "\n",
    "4. **Considerations**:\n",
    "\n",
    "   - **Domain Knowledge**: It's important to involve domain knowledge when selecting features. Domain experts can provide insights into which features are likely to be relevant and meaningful for the problem at hand.\n",
    "   \n",
    "   - **Evaluation Metrics**: The choice of evaluation metrics, such as accuracy, precision, recall, or F1-score, depends on the nature of the problem. The selected features should improve the chosen metric.\n",
    "\n",
    "   - **Trade-offs**: While feature selection offers benefits, it's important to strike a balance. Removing too many features can lead to information loss, while including irrelevant features can degrade model performance. Cross-validation can help identify the optimal number of features to include.\n",
    "\n",
    "In conclusion, feature selection is a critical technique for addressing the curse of dimensionality by reducing the number of features while retaining or enhancing the information necessary for accurate model predictions. It leads to more efficient, interpretable, and generalizable machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794bc431-9e4b-49fd-be55-97c9a54633bf",
   "metadata": {},
   "source": [
    "Q5. What are some limitations and drawbacks of using dimensionality reduction techniques in machine\n",
    "learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ec3b0d-c895-46a1-8750-6f8f6a81f4ae",
   "metadata": {},
   "source": [
    "Dimensionality reduction techniques are valuable tools in machine learning for addressing the curse of dimensionality and improving model performance. However, they also come with limitations and potential drawbacks that need to be considered:\n",
    "\n",
    "1. **Information Loss**: Dimensionality reduction often involves projecting high-dimensional data onto a lower-dimensional subspace. This projection can lead to information loss, as some of the original variance and relationships between features might not be fully captured in the reduced space. The challenge is to balance dimensionality reduction with preserving the most important information.\n",
    "\n",
    "2. **Interpretability**: While dimensionality reduction can simplify data, it can also make the data less interpretable. The reduced dimensions might not directly correspond to meaningful or easily understandable features, making it harder to explain the relationships within the data.\n",
    "\n",
    "3. **Algorithm Sensitivity**: Different dimensionality reduction algorithms have different assumptions and behaviors. The choice of algorithm can significantly impact the outcomes. Some algorithms might work better with certain types of data distributions or patterns, while others might be less effective.\n",
    "\n",
    "4. **Curse of Interpretability**: The \"curse of interpretability\" refers to the difficulty of interpreting the results of dimensionality reduction, especially when the data is transformed into a space with fewer dimensions. Visualizing data in high dimensions is challenging, and understanding the new feature representations can be complex.\n",
    "\n",
    "5. **Computation and Scalability**: Some dimensionality reduction techniques, particularly nonlinear ones like t-SNE or manifold learning, can be computationally expensive and time-consuming, especially for large datasets. This can make them less practical for real-time or large-scale applications.\n",
    "\n",
    "6. **Applicability to Out-of-Sample Data**: When applying dimensionality reduction techniques to new, unseen data (out-of-sample data), there's a risk that the reduction might not perform as well as it did during training. The reduced space might not capture the same patterns or relationships in the new data.\n",
    "\n",
    "7. **Parameter Tuning**: Many dimensionality reduction algorithms have hyperparameters that need to be tuned. Selecting the optimal hyperparameters can be a challenge, and improper tuning can lead to suboptimal results.\n",
    "\n",
    "8. **Linear vs. Nonlinear Relationships**: Some dimensionality reduction techniques, like PCA, assume linear relationships between features. If the underlying relationships are nonlinear, linear techniques might not capture the most important variations in the data.\n",
    "\n",
    "9. **Curse of Dimensionality for High-Dimensional Reduction**: Even dimensionality reduction techniques themselves can be affected by the curse of dimensionality in high-dimensional data. The performance of these techniques can degrade when applied to data with extremely high dimensions.\n",
    "\n",
    "10. **Prone to Noise**: Dimensionality reduction techniques can be sensitive to noise in the data. Noise might be amplified during the reduction process, leading to suboptimal results.\n",
    "\n",
    "11. **Choosing the Right Technique**: Selecting the appropriate dimensionality reduction technique for a given problem requires an understanding of the data distribution, the goals of the analysis, and the algorithm's assumptions.\n",
    "\n",
    "In conclusion, while dimensionality reduction techniques are powerful tools for addressing challenges associated with high-dimensional data, practitioners should be aware of their limitations and potential drawbacks. Careful consideration, experimentation, and validation are essential to ensure that dimensionality reduction improves the overall quality of the machine learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58c3910-1da3-41fa-815e-447dec715d49",
   "metadata": {},
   "source": [
    "Q6. How does the curse of dimensionality relate to overfitting and underfitting in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729987c7-03f7-4bbb-87c3-999474e77d2e",
   "metadata": {},
   "source": [
    "The curse of dimensionality is closely related to the concepts of overfitting and underfitting in machine learning. These concepts are all intertwined through the effect of increasing dimensionality on model performance. Let's explore how they are connected:\n",
    "\n",
    "1. **Curse of Dimensionality and Overfitting**:\n",
    "   - The curse of dimensionality refers to the challenges that arise when working with high-dimensional data, such as data becoming sparse, distances becoming distorted, and computational complexity increasing.\n",
    "   - Overfitting occurs when a model learns noise or random fluctuations in the training data rather than the underlying patterns. This leads to a model that performs well on the training data but poorly on new, unseen data.\n",
    "   - High-dimensional data exacerbates the risk of overfitting. With more dimensions, the model has more parameters to fit the data, increasing the likelihood of capturing noise. The increased complexity can lead the model to memorize the training data rather than generalize.\n",
    "\n",
    "2. **Curse of Dimensionality and Underfitting**:\n",
    "   - Underfitting occurs when a model is too simple to capture the underlying patterns in the data. It performs poorly on both the training data and new data because it fails to learn the relevant relationships.\n",
    "   - The curse of dimensionality can contribute to underfitting as well. When data is high-dimensional, simple models might struggle to find meaningful relationships due to the increased complexity of the feature space.\n",
    "\n",
    "3. **Balancing Model Complexity**:\n",
    "   - Both overfitting and underfitting are linked to the balance between model complexity and the amount of data available. In low-dimensional data, increasing model complexity might improve performance up to a point. However, in high-dimensional data, the increase in complexity can lead to overfitting more quickly.\n",
    "   - As dimensionality increases, it becomes more challenging to find the right balance of model complexity. Very complex models risk overfitting, while overly simple models risk underfitting.\n",
    "\n",
    "4. **Addressing Overfitting and Underfitting in High Dimensions**:\n",
    "   - To address overfitting caused by the curse of dimensionality, regularization techniques become crucial. Regularization penalizes overly complex models, discouraging them from fitting noise.\n",
    "   - Feature selection and dimensionality reduction help combat both overfitting and underfitting by reducing the number of features or dimensions while retaining relevant information.\n",
    "\n",
    "5. **Role of Cross-Validation**:\n",
    "   - Cross-validation is essential for detecting overfitting and underfitting in the presence of high-dimensional data. It helps assess the generalization performance of models by evaluating their performance on unseen data.\n",
    "\n",
    "In summary, the curse of dimensionality interacts with overfitting and underfitting in complex ways. High-dimensional data increases the risk of overfitting due to increased model complexity and the presence of noise. Additionally, underfitting can occur when models struggle to capture meaningful relationships in the high-dimensional space. Proper model selection, regularization, feature selection, and dimensionality reduction are essential strategies to navigate these challenges and achieve models that generalize well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84b3079-d86c-4bf2-a13c-70213fdf5285",
   "metadata": {},
   "source": [
    "Q7. How can one determine the optimal number of dimensions to reduce data to when using\n",
    "dimensionality reduction techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fbc168-9e76-4fa7-9ee0-82c20ab7e65e",
   "metadata": {},
   "source": [
    "Determining the optimal number of dimensions to reduce data to is an important but often challenging task when using dimensionality reduction techniques. The optimal number of dimensions strikes a balance between retaining enough information to capture meaningful patterns while avoiding overfitting or excessive complexity. Here are several approaches you can use to determine the optimal number of dimensions:\n",
    "\n",
    "1. **Explained Variance Ratio**:\n",
    "   - For techniques like Principal Component Analysis (PCA), the explained variance ratio provides insight into how much variance in the data is retained by each principal component.\n",
    "   - Plot the cumulative explained variance as a function of the number of dimensions. Choose a threshold (e.g., 95% variance retained) and select the number of dimensions where the cumulative variance crosses or exceeds that threshold.\n",
    "\n",
    "2. **Scree Plot**:\n",
    "   - In PCA, the scree plot shows the eigenvalues of the principal components. A sudden drop in eigenvalues indicates the point where adding more dimensions contributes less to explaining the variance.\n",
    "   - Select the number of dimensions corresponding to the point where the eigenvalues start to level off.\n",
    "\n",
    "3. **Cross-Validation**:\n",
    "   - Use cross-validation to evaluate the performance of your machine learning model with varying numbers of dimensions.\n",
    "   - Plot the model's performance (e.g., accuracy or mean squared error) against the number of dimensions. Look for the point where the performance stabilizes or starts to degrade.\n",
    "\n",
    "4. **Reconstruction Error**:\n",
    "   - For techniques like Autoencoders, measure the reconstruction error of the reduced data compared to the original data.\n",
    "   - Plot the reconstruction error as a function of the number of dimensions. Choose a threshold (e.g., a certain level of acceptable error increase) to determine the optimal number of dimensions.\n",
    "\n",
    "5. **Domain Knowledge**:\n",
    "   - Incorporate domain knowledge to guide your decision. If you have insights about the data and its underlying patterns, you can make an informed choice about the number of dimensions that captures the most relevant information.\n",
    "\n",
    "6. **Visualization**:\n",
    "   - If possible, visualize the data in the reduced-dimensional space for different numbers of dimensions.\n",
    "   - Choose the number of dimensions that provide a clear and interpretable visualization of the data's structure.\n",
    "\n",
    "7. **Algorithm Performance**:\n",
    "   - If your ultimate goal is to apply a machine learning algorithm to the reduced data, use the performance of that algorithm on validation or test data to determine the optimal number of dimensions.\n",
    "\n",
    "8. **Information Criteria**:\n",
    "   - Information criteria like AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) can be used for model selection, including the number of dimensions in dimensionality reduction.\n",
    "\n",
    "Remember that there might not be a single \"optimal\" number of dimensions that universally fits all situations. It often requires experimentation and a combination of multiple approaches to determine the most suitable number of dimensions for your specific problem. Regularization techniques can also help mitigate the impact of choosing an overly large number of dimensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
