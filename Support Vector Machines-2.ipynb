{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b2e00f7-c6c6-475a-8ee6-70d96f731ecd",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bab1d33-2e37-4416-9e03-3e5508c8a8da",
   "metadata": {},
   "source": [
    "In machine learning algorithms, kernel functions play a significant role in transforming data into a higher-dimensional space, often enabling algorithms to capture more complex patterns that might not be linearly separable in the original feature space. Polynomial functions are a type of kernel function used for this purpose.\n",
    "\n",
    "A polynomial kernel is a type of kernel function that computes the similarity between two data points by taking the dot product of their feature vectors raised to a certain power (degree). The polynomial kernel is defined as:\n",
    "\n",
    "K(x, y) = (α * (x ∙ y) + c)^d\n",
    "\n",
    "Where:\n",
    "- x and y are the feature vectors of two data points.\n",
    "- α is a coefficient that controls the influence of the dot product term.\n",
    "- c is a constant.\n",
    "- d is the degree of the polynomial.\n",
    "\n",
    "The polynomial kernel is a way of implicitly projecting the data into a higher-dimensional space without actually computing the transformation explicitly. It allows algorithms like Support Vector Machines (SVM) to learn non-linear decision boundaries in the original feature space.\n",
    "\n",
    "The relationship between polynomial functions and kernel functions is that polynomial kernels are a specific type of kernel function that employs a polynomial transformation to compute the similarity between data points. They are used in various machine learning algorithms, including SVMs, to handle non-linear data and find complex decision boundaries.\n",
    "\n",
    "Other types of kernel functions, such as Gaussian (RBF) kernels, sigmoid kernels, and more, also serve similar purposes of mapping data to higher-dimensional spaces. Different kernel functions are chosen based on the characteristics of the data and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1b21c5-f0b7-4798-abf4-2d64d8c5f7d9",
   "metadata": {},
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cc5cff-6717-41c2-9163-ecab3159f49c",
   "metadata": {},
   "source": [
    "Step 1: Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d838d68d-a65c-4e33-9559-b8066023a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5f523b-fa7d-4689-90cf-7d64aa05188d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Step 2: Generate a sample dataset for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9ae6544-34af-4e9a-abf3-fb39d48127cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100, n_features=10, n_informative=5, n_classes=2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb050d20-cd05-449f-b7f6-3622e3704273",
   "metadata": {},
   "source": [
    "Step 3: Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1ab071f-f516-4140-b7c6-905ae5785946",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1c5bea-6612-489c-a4ce-50218c0ddf1f",
   "metadata": {},
   "source": [
    "Step 4: Create an instance of the SVM model with the polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5234466d-9873-4d1e-93c1-1ee7dbbf6a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_svm = SVC(kernel='poly', degree=3, gamma='scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b62f1f-00ae-4798-b81a-2bc5e917ba48",
   "metadata": {},
   "source": [
    "Step 5: Fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2d1f99c-684e-4620-8039-68fd58a1f215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a5951-ca6d-4801-90d8-1809b4c03a20",
   "metadata": {},
   "source": [
    "Step 6: Make predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "173d91ef-b01f-421f-a8c3-f597d52f2d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = poly_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a448b7f6-1408-4b47-bf9c-29ad9722f5aa",
   "metadata": {},
   "source": [
    "Step 7: Evaluate the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "428cd7d2-4b62-481c-ad63-4de2b9d74246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd4141c-642f-41d3-bedb-70e401952602",
   "metadata": {},
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c853d1-82fd-4ea3-a23f-bbea6197f312",
   "metadata": {},
   "source": [
    "In machine learning algorithms, kernel functions play a significant role in transforming data into a higher-dimensional space, often enabling algorithms to capture more complex patterns that might not be linearly separable in the original feature space. Polynomial functions are a type of kernel function used for this purpose.\n",
    "\n",
    "A polynomial kernel is a type of kernel function that computes the similarity between two data points by taking the dot product of their feature vectors raised to a certain power (degree). The polynomial kernel is defined as:\n",
    "\n",
    "K(x, y) = (α * (x ∙ y) + c)^d\n",
    "\n",
    "Where:\n",
    "- x and y are the feature vectors of two data points.\n",
    "- α is a coefficient that controls the influence of the dot product term.\n",
    "- c is a constant.\n",
    "- d is the degree of the polynomial.\n",
    "\n",
    "The polynomial kernel is a way of implicitly projecting the data into a higher-dimensional space without actually computing the transformation explicitly. It allows algorithms like Support Vector Machines (SVM) to learn non-linear decision boundaries in the original feature space.\n",
    "\n",
    "The relationship between polynomial functions and kernel functions is that polynomial kernels are a specific type of kernel function that employs a polynomial transformation to compute the similarity between data points. They are used in various machine learning algorithms, including SVMs, to handle non-linear data and find complex decision boundaries.\n",
    "\n",
    "Other types of kernel functions, such as Gaussian (RBF) kernels, sigmoid kernels, and more, also serve similar purposes of mapping data to higher-dimensional spaces. Different kernel functions are chosen based on the characteristics of the data and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931a0118-5a21-4ef2-b5a0-c89f56575f12",
   "metadata": {},
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93bdfaa-b494-488a-9204-afd60602bcfb",
   "metadata": {},
   "source": [
    "Support Vector Regression (SVR) is a powerful technique for regression tasks, and the choice of various parameters can significantly impact its performance. Let's discuss how the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affects SVR's performance:\n",
    "\n",
    "1. **Kernel Function:**\n",
    "   Kernel functions allow SVR to implicitly operate in a higher-dimensional space without explicitly calculating the transformations. Common kernel functions are Linear, Polynomial, Radial Basis Function (RBF), and Sigmoid. The choice of kernel depends on the data distribution and complexity:\n",
    "   - **Linear Kernel:** Suitable for linear relationships.\n",
    "   - **Polynomial Kernel:** Useful for capturing polynomial relationships. Increase the degree to model higher-degree polynomials.\n",
    "   - **RBF Kernel:** Suitable for capturing non-linear relationships. Increase gamma to make the model more sensitive to training data.\n",
    "   - **Sigmoid Kernel:** Used for non-linear data. Adjust gamma and coefficients to control the shape of the decision boundary.\n",
    "\n",
    "2. **C Parameter (Regularization):**\n",
    "   The C parameter balances the trade-off between maximizing the margin and minimizing the training error. It controls the importance of misclassified points:\n",
    "   - **Small C:** Emphasizes a wider margin, but allows more margin violations (misclassifications). Useful when you want to tolerate some errors.\n",
    "   - **Large C:** Emphasizes accurate classification of data points. Results in a narrower margin and fewer margin violations. Useful when you want to prioritize correct classification.\n",
    "\n",
    "3. **Epsilon Parameter:**\n",
    "   Epsilon defines the width of the tube around the regression line within which errors are not penalized:\n",
    "   - **Smaller Epsilon:** Focuses on fitting data points more closely.\n",
    "   - **Larger Epsilon:** Allows more tolerance for errors, leading to a wider margin.\n",
    "\n",
    "4. **Gamma Parameter (Kernel Coefficient):**\n",
    "   The gamma parameter affects the shape of the decision boundary for kernel functions. It controls the influence of each training point:\n",
    "   - **Small Gamma:** Produces a broader curve, making the decision boundary less sensitive to variations. Suitable for smoother data.\n",
    "   - **Large Gamma:** Creates a sharper curve, making the decision boundary more sensitive to variations. Suitable for complex and noisy data.\n",
    "\n",
    "Examples of tuning these parameters:\n",
    "- **Increasing C:** If your data is well-behaved and you want precise classification, increase C.\n",
    "- **Increasing Gamma:** Use a higher gamma for non-linear data with complex relationships.\n",
    "- **Increasing Epsilon:** Use a larger epsilon if you want to allow more errors and prioritize a wider margin.\n",
    "- **Choosing Kernel:** If the data is non-linear, experiment with different kernels to find the one that captures the underlying pattern.\n",
    "\n",
    "It's important to perform hyperparameter tuning using techniques like cross-validation to find the optimal combination of parameters for your specific problem. The impact of parameter changes can vary from dataset to dataset, so experimentation is key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4cbd52-07db-4338-af52-d641bc80f7c9",
   "metadata": {},
   "source": [
    "Q5. Assignment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0955b629-456a-49c0-ae5c-2fd40b13fdc6",
   "metadata": {},
   "source": [
    "1. Import the necessary libraries and load the dataseg\n",
    "2. Split the dataset into training and testing setZ\n",
    "3. Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "4. Create an instance of the SVC classifier and train it on the training datW\n",
    "5. hse the trained classifier to predict the labels of the testing datW\n",
    "6. Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK\n",
    "7. Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "improve its performanc_\n",
    "8. Train the tuned classifier on the entire dataseg\n",
    "9. Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84e7bce-8630-46e8-a05c-13d072edc6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
