{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c79556b-f2f3-453d-b26c-e73d12056671",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a7bcf1-0023-429b-bcd4-0c51f463d900",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for both regression and classification tasks. It's an ensemble learning method that combines the predictions of multiple individual models (usually decision trees) to create a stronger overall predictive model. The basic idea behind gradient boosting is to iteratively build new models that focus on correcting the errors of the previous models.\n",
    "\n",
    "Here's how gradient boosting regression works:\n",
    "\n",
    "1. **Base Model Selection**: The process starts with an initial simple model, often a decision tree with just a few levels (a shallow tree), which is called the \"base model\" or \"weak learner.\"\n",
    "\n",
    "2. **Residual Calculation**: The predictions from the base model are compared to the actual target values, and the differences between the predictions and the actual values (residuals) are calculated.\n",
    "\n",
    "3. **Next Model Creation**: A new model is then trained to predict these residuals. This new model is designed to capture the patterns in the residuals left unexplained by the previous model.\n",
    "\n",
    "4. **Combining Predictions**: The predictions of the new model are added to the predictions of the previous models. This combined prediction is expected to be closer to the actual target values.\n",
    "\n",
    "5. **Iteration**: Steps 2-4 are repeated for a specified number of iterations (or until a certain level of performance is achieved). In each iteration, a new model is trained to predict the residuals of the combined predictions from the previous models.\n",
    "\n",
    "6. **Final Prediction**: The final prediction is obtained by summing up the predictions of all the individual models.\n",
    "\n",
    "Gradient boosting adjusts the predictions of each successive model in a way that it tries to reduce the error of the previous models. This is achieved by using a gradient descent optimization algorithm to find the best parameters for each new model.\n",
    "\n",
    "Popular libraries like XGBoost, LightGBM, and Scikit-learn's GradientBoostingRegressor provide efficient implementations of gradient boosting regression. These libraries allow you to tune various hyperparameters to control the behavior of the boosting process, such as the learning rate, the depth of each tree, and the number of boosting iterations.\n",
    "\n",
    "Gradient Boosting Regression often produces accurate models, but it's important to carefully tune the hyperparameters to avoid overfitting and achieve the best possible performance on the specific problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268750a-b9bc-44df-8934-0d308cd0cf02",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d89a3b8e-dfc0-4c20-b0cb-127c9cc04adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37f8dd9e-1045-4c24-8ca8-546058d49341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0034\n",
      "R-squared: 0.9895\n"
     ]
    }
   ],
   "source": [
    "# Generate some sample data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(50, 1)\n",
    "y = 2 * X.squeeze() + 1 + 0.1 * np.random.randn(50)\n",
    "\n",
    "# Define the number of boosting iterations\n",
    "n_iterations = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize predictions with the mean of the target values\n",
    "predictions = np.full_like(y, np.mean(y))\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Calculate residuals (negative gradient)\n",
    "    residuals = y - predictions\n",
    "    \n",
    "    # Fit a regression tree to the residuals\n",
    "    tree = DecisionTreeRegressor(max_depth=1)\n",
    "    tree.fit(X, residuals)\n",
    "    \n",
    "    # Make predictions with the tree\n",
    "    tree_predictions = tree.predict(X)\n",
    "    \n",
    "    # Update predictions with the scaled predictions from the tree\n",
    "    predictions += learning_rate * tree_predictions\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, predictions)\n",
    "r2 = r2_score(y, predictions)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e98c641-c58f-450a-9039-08538d55bfbb",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7f6ab3-22b4-4ba9-9e25-b09375e05e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 50}\n",
      "Mean Squared Error: 1.5235\n",
      "R-squared: 0.9989\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Generate some sample data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [1, 2, 3],\n",
    "}\n",
    "\n",
    "# Create the GradientBoostingRegressor model\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions using the best estimator\n",
    "y_pred = best_estimator.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6368c4fc-0785-4f7b-9dee-de315275a4da",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc3df1e-8ab9-4afb-a735-947595c263e5",
   "metadata": {},
   "source": [
    "In the context of gradient boosting, a weak learner (also referred to as a weak base learner) is a simple and relatively low-complexity model that performs slightly better than random guessing on a given task. In the context of regression, a weak learner could be a decision tree with a shallow depth (only a few levels) or even a linear regression model.\n",
    "\n",
    "The concept of weak learners is fundamental to the success of gradient boosting. Gradient boosting works by iteratively combining the predictions of weak learners to create a strong predictive model. In each iteration, a new weak learner is trained to correct the errors of the combined predictions from the previous models. These weak learners, though individually weak, when combined in an ensemble, can lead to a powerful and accurate predictive model.\n",
    "\n",
    "The term \"weak\" is not meant to imply that these models are entirely ineffective, but rather that they are intentionally kept simple and constrained to capture only a small portion of the underlying patterns in the data. The strength of gradient boosting lies in its ability to focus on the residual errors of the previous models and to gradually improve the predictions by iteratively adjusting the new models to address the remaining errors.\n",
    "\n",
    "Common examples of weak learners used in gradient boosting include:\n",
    "\n",
    "1. **Decision Stumps or Trees**: Shallow decision trees with only a few levels. These trees make simple binary decisions based on a single feature.\n",
    "\n",
    "2. **Linear Models**: Linear regression models that fit a linear relationship between the features and the target.\n",
    "\n",
    "3. **Polynomial Models**: Simple polynomial regression models capturing higher-order relationships.\n",
    "\n",
    "Weak learners are computationally efficient to train and are designed to be quick to compute. The iterative nature of gradient boosting allows it to focus on complex patterns that may not be easily captured by a single weak learner. As more weak learners are added to the ensemble, the boosting algorithm adapts their individual contributions to collectively improve the overall model's performance.\n",
    "\n",
    "In summary, weak learners in gradient boosting are intentionally basic models that, when combined strategically, form a powerful ensemble capable of tackling complex predictive tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996e79a3-b5c1-40ec-bfb4-e75a8078f8a1",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abad638-23b2-446c-acb0-3e64da429d36",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be understood through the metaphor of a team of \"experts\" collaborating to solve a problem. Each expert, represented by a weak learner, focuses on correcting the mistakes of the previous experts, resulting in a final \"consensus\" prediction that is often much better than that of any individual expert.\n",
    "\n",
    "Here's the step-by-step intuition behind the Gradient Boosting algorithm:\n",
    "\n",
    "1. **Starting Point**: Imagine you have a problem to solve, and you start with an initial guess (prediction) that might not be very accurate. This initial guess could be as simple as the mean of the target values in regression tasks.\n",
    "\n",
    "2. **Identifying Errors**: You identify the errors (residuals) between your initial guess and the actual target values. These errors represent what your current guess got wrong.\n",
    "\n",
    "3. **Training a Weak Learner**: You train a weak learner, often a shallow decision tree, to predict these errors. This new model is designed to focus on the patterns that your initial guess failed to capture.\n",
    "\n",
    "4. **Correcting Mistakes**: You combine the predictions of your initial guess and the new weak learner. By adding the predictions from the weak learner to your initial guess, you hope to move closer to the correct answer.\n",
    "\n",
    "5. **Iterative Process**: The process repeats. You identify the errors between the combined prediction and the actual targets, then train a new weak learner to predict these errors. Again, you combine the predictions to correct your mistakes.\n",
    "\n",
    "6. **Accumulated Expertise**: With each iteration, the weak learners learn to focus on different aspects of the problem, gradually reducing the errors and improving the overall prediction. It's like having a team of experts, each specializing in a different aspect of the problem, collaborating to reach a consensus.\n",
    "\n",
    "7. **Optimization**: The process continues for a specified number of iterations or until a stopping criterion is met. At the end, the final prediction is the accumulated prediction from all the weak learners.\n",
    "\n",
    "The key idea is that by iteratively building and combining weak learners, Gradient Boosting adapts to the errors made by the previous models, effectively reducing those errors over time. This process of correcting mistakes leads to a stronger and more accurate overall model.\n",
    "\n",
    "The term \"gradient\" in Gradient Boosting comes from the optimization technique used to find the best parameters for each new weak learner. The algorithm adjusts the new model's parameters in the direction that minimizes the gradient of the loss function (errors) with respect to the current predictions. This way, each new model contributes to reducing the errors made by the ensemble of previous models.\n",
    "\n",
    "In summary, Gradient Boosting leverages the collaborative strength of weak learners to iteratively refine predictions and build a powerful ensemble model that can capture complex patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edf44ad-1005-47d9-bb20-a51791cbc478",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f2e0b-f71e-4708-828a-8246f782ffdd",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners through an iterative process. The key idea is to improve the overall model's predictive power by focusing each weak learner on the errors or residuals left by the previous learners. This process gradually reduces the errors and produces a strong ensemble model. Here's how the algorithm builds the ensemble:\n",
    "\n",
    "1. **Initialize the Ensemble**:\n",
    "   - Start with an initial prediction, often the mean of the target values.\n",
    "   - Calculate the residuals (errors) between these initial predictions and the actual target values.\n",
    "\n",
    "2. **Iterative Process**:\n",
    "   - For each iteration (or boosting round):\n",
    "     - Train a new weak learner (often a shallow decision tree) on the residuals from the previous round. The goal is to predict the errors made by the previous ensemble of models.\n",
    "     - Update the ensemble's prediction by adding a scaled version of the new weak learner's predictions. The scaling factor is the learning rate, which controls the contribution of each new model to the ensemble.\n",
    "     - The updated ensemble prediction now includes the knowledge of the previous models plus the new model's ability to correct their errors.\n",
    "\n",
    "3. **Repeat Iterations**:\n",
    "   - Repeat the iterative process for a specified number of rounds or until a stopping criterion is met.\n",
    "   - With each iteration, the new weak learner's focus shifts towards capturing the errors that remain unexplained by the ensemble so far.\n",
    "\n",
    "4. **Final Ensemble**:\n",
    "   - After all iterations, the ensemble consists of the sum of predictions from all the weak learners.\n",
    "   - The final ensemble prediction is a more accurate and robust prediction compared to any individual weak learner.\n",
    "\n",
    "The magic of Gradient Boosting lies in its ability to optimize the parameters of each weak learner in a way that minimizes the errors of the combined ensemble. This optimization is done using gradient descent. In each iteration, the algorithm calculates the negative gradient (residuals) of the loss function with respect to the current ensemble's predictions. The new weak learner is then trained to minimize this gradient, effectively focusing on reducing the errors of the ensemble.\n",
    "\n",
    "By iteratively building and combining weak learners, each focusing on different aspects of the problem, Gradient Boosting adapts to the errors made by the previous models and gradually refines its predictions. This iterative approach leads to improved predictive accuracy and the ability to capture complex relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d37ea2f-b6b2-45c3-aaf2-9aee5cd630bc",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b072fe1-5ea8-401b-b74b-2bb879e0d40e",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition of the Gradient Boosting algorithm involves understanding the optimization process and how the algorithm minimizes the loss function by iteratively adding weak learners. Here are the key steps in the mathematical intuition of Gradient Boosting:\n",
    "\n",
    "1. **Loss Function and Residuals**:\n",
    "   - The process starts with a loss function that quantifies the difference between the model's predictions and the actual target values. Common loss functions for regression tasks include mean squared error (MSE) and absolute error (L1 loss).\n",
    "   - Calculate the negative gradient of the loss function with respect to the current ensemble's predictions. This negative gradient represents the residuals or errors that the current ensemble makes on the training data.\n",
    "\n",
    "2. **Initializing Ensemble**:\n",
    "   - Begin with an initial prediction (often the mean of target values) and initialize the ensemble.\n",
    "   - Calculate the residuals by subtracting the initial prediction from the actual target values.\n",
    "\n",
    "3. **Iterative Optimization**:\n",
    "   - For each boosting iteration:\n",
    "     - Train a new weak learner (e.g., decision tree) on the residuals from the previous iteration. The goal is to predict the residuals that the current ensemble is not capturing well.\n",
    "     - Update the ensemble's predictions by adding the scaled predictions from the new weak learner. The scaling factor is the learning rate, which controls the contribution of the new model to the ensemble.\n",
    "     - Calculate the negative gradient of the loss function with respect to the updated ensemble predictions. These new residuals are the errors that the current ensemble plus the new model are making.\n",
    "\n",
    "4. **Optimization of Weak Learner**:\n",
    "   - Train the new weak learner to minimize the negative gradient calculated in the previous step. This means the weak learner is fitted to the task of predicting the errors of the current ensemble.\n",
    "\n",
    "5. **Ensemble Update**:\n",
    "   - Update the ensemble by adding the predictions from the new weak learner. The updated ensemble prediction now includes the knowledge of the previous models plus the new model's ability to correct their errors.\n",
    "\n",
    "6. **Repeat Iterations**:\n",
    "   - Continue the iterative process for a specified number of rounds or until a stopping criterion is met.\n",
    "   - In each iteration, the new weak learner focuses on the errors that remain unexplained by the current ensemble.\n",
    "\n",
    "7. **Final Ensemble**:\n",
    "   - After all iterations, the final ensemble consists of the sum of predictions from all the weak learners.\n",
    "   - This ensemble prediction is optimized to minimize the loss function on the training data.\n",
    "\n",
    "8. **Prediction and Generalization**:\n",
    "   - Use the final ensemble to make predictions on new, unseen data.\n",
    "   - The ensemble's ability to generalize well is a result of its collaborative efforts to correct and improve upon the mistakes of the individual weak learners.\n",
    "\n",
    "The mathematical intuition of Gradient Boosting revolves around minimizing the loss function by iteratively correcting errors through the addition of weak learners. Each iteration adjusts the ensemble's predictions to focus on previously unexplained errors, resulting in an increasingly accurate and powerful predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab7fbc-4946-4354-ad61-55e2d5d310ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
