{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact the validity of the results.**"
      ],
      "metadata": {
        "id": "Y9-LEMvyk89j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANOVA (Analysis of Variance) is a statistical test used to compare the means of three or more groups. To use ANOVA and ensure the validity of its results, several assumptions need to be met. Here are the assumptions required for ANOVA:\n",
        "\n",
        "1. Independence: The observations within each group should be independent of each other. This assumption implies that the data points in one group should not be influenced by or correlated with the data points in another group.\n",
        "\n",
        "2. Normality: The data within each group should follow a normal distribution. Normality assumption means that the residuals (the differences between the observed values and the predicted values) should be normally distributed.\n",
        "\n",
        "3. Homogeneity of Variance (Homoscedasticity): The variability or dispersion of the data should be similar across all groups. Homogeneity of variance means that the standard deviation of the dependent variable is equal across all groups.\n",
        "\n",
        "4. Equal Sample Sizes (for two-way and three-way ANOVA): In two-way and three-way ANOVA, if there are multiple factors or independent variables, it is assumed that the sample sizes are equal across all combinations of factors. This assumption ensures balanced designs.\n",
        "\n",
        "Violations of these assumptions can impact the validity of the ANOVA results. Here are examples of violations and their impact:\n",
        "\n",
        "1. Violation of Independence: If there is dependence or correlation between observations in different groups, it violates the independence assumption. For example, if participants in one group are related to each other (e.g., family members), the assumption of independence is violated. Violations of independence can inflate Type I error rates, leading to incorrect conclusions.\n",
        "\n",
        "2. Violation of Normality: If the data within groups do not follow a normal distribution, ANOVA results may be unreliable. Non-normality can affect the accuracy of p-values and confidence intervals. Violations of normality can occur when there are outliers or skewness in the data.\n",
        "\n",
        "3. Violation of Homogeneity of Variance: If the variability of the data differs significantly across groups, the assumption of homogeneity of variance is violated. This violation can lead to incorrect conclusions about the significance of group differences. Violations of homogeneity of variance can result in inflated or deflated Type I error rates.\n",
        "\n",
        "4. Violation of Equal Sample Sizes: In two-way and three-way ANOVA, unequal sample sizes across combinations of factors can impact the interpretation of the interaction effects. Unequal sample sizes can lead to imbalanced designs and affect the validity of the ANOVA results.\n",
        "\n",
        "It is important to check these assumptions before conducting ANOVA and, if violated, consider alternative statistical methods or transformations to address the violations."
      ],
      "metadata": {
        "id": "qSJ2VBSmoLE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What are the three types of ANOVA, and in what situations would each be used?**"
      ],
      "metadata": {
        "id": "KPwsvoPFk84H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The three types of ANOVA are:\n",
        "\n",
        "1. One-Way ANOVA: One-Way ANOVA is used when you have one independent variable or factor with three or more levels or groups. It is used to determine if there are any statistically significant differences between the means of the groups. For example, if you want to compare the mean scores of students from different schools (e.g., School A, School B, School C), you can use One-Way ANOVA.\n",
        "\n",
        "2. Two-Way ANOVA: Two-Way ANOVA is used when you have two independent variables or factors and want to examine the main effects of each factor as well as the interaction between them. It allows you to investigate if there are significant differences between groups based on the factors independently and if there is an interaction effect between the factors. For example, if you want to analyze the effects of both gender and age group on test scores, you can use Two-Way ANOVA.\n",
        "\n",
        "3. Three-Way ANOVA: Three-Way ANOVA is used when you have three independent variables or factors and want to analyze the main effects and interaction effects among all three factors. It allows you to explore the simultaneous effects of three factors and their interactions. For example, if you want to examine the effects of treatment type, dosage, and gender on patient outcomes, you can use Three-Way ANOVA.\n",
        "\n",
        "In summary, One-Way ANOVA is used when you have one factor, Two-Way ANOVA is used when you have two factors, and Three-Way ANOVA is used when you have three factors. Each type of ANOVA enables the comparison of means between groups or the exploration of main effects and interactions among factors. The choice of ANOVA type depends on the number of factors and research objectives."
      ],
      "metadata": {
        "id": "TJctLQ3qoNIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?**"
      ],
      "metadata": {
        "id": "x5ZOpWVUk8zf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The partitioning of variance in ANOVA refers to the decomposition of the total variance observed in the data into different components based on the sources of variation. This decomposition helps understand the relative contributions of different factors or sources of variation to the overall variability in the data. It is a fundamental concept in ANOVA that allows us to quantify the extent to which factors or variables explain the observed differences in means.\n",
        "\n",
        "The partitioning of variance in ANOVA involves dividing the total variance into three components:\n",
        "\n",
        "1. Between-Group Variance: This component represents the variability between the group means. It measures the differences between the group means and indicates the extent to which the independent variable or factor explains the variation in the dependent variable.\n",
        "\n",
        "2. Within-Group Variance: This component represents the variability within each group. It measures the variability of individual data points within each group and reflects the random or unexplained variation that is not accounted for by the independent variable.\n",
        "\n",
        "3. Total Variance: This component represents the overall variability in the data. It is the sum of the between-group variance and the within-group variance. It provides a measure of the total variation observed in the dependent variable.\n",
        "\n",
        "Understanding the partitioning of variance in ANOVA is important for several reasons:\n",
        "\n",
        "1. Hypothesis Testing: ANOVA allows us to test whether the observed differences between groups are statistically significant. By understanding the partitioning of variance, we can assess the proportion of the total variance that can be attributed to the factor of interest, helping us determine the significance of the factor.\n",
        "\n",
        "2. Effect Size: The partitioning of variance allows us to compute effect size measures such as eta-squared or omega-squared. These measures indicate the proportion of variance accounted for by the factor or factors in ANOVA, providing information about the strength or magnitude of the effect.\n",
        "\n",
        "3. Study Design: Understanding the partitioning of variance aids in designing future studies. It helps identify which factors or sources of variation contribute the most to the overall variability, allowing researchers to focus on the most influential factors and optimize their study designs accordingly.\n",
        "\n",
        "4. Interpretation: By decomposing the total variance into components, we gain insights into the relative importance of different factors or variables in explaining the variation in the dependent variable. This understanding enhances the interpretation of the results and allows for more meaningful discussions of the study findings.\n",
        "\n",
        "In summary, the partitioning of variance in ANOVA is crucial for hypothesis testing, effect size estimation, study design optimization, and meaningful interpretation of the results. It provides a structured approach to understand and quantify the sources of variation in the data."
      ],
      "metadata": {
        "id": "yAqJ5zuMoPfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. How would you calculate the total sum of  squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python?**"
      ],
      "metadata": {
        "id": "TD0BEJp0k8un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "# Define the data for the groups\n",
        "group_1 = [10, 12, 14, 15, 18]\n",
        "group_2 = [8, 9, 11, 13, 15]\n",
        "group_3 = [7, 8, 9, 11, 12]\n",
        "\n",
        "# Combine the data into a single array\n",
        "data = group_1 + group_2 + group_3\n",
        "\n",
        "# Create the corresponding group labels\n",
        "groups = ['Group 1'] * len(group_1) + ['Group 2'] * len(group_2) + ['Group 3'] * len(group_3)\n",
        "\n",
        "# Fit the one-way ANOVA model\n",
        "model = ols('data ~ groups', data=data).fit()\n",
        "anova_table = sm.stats.anova_lm(model)\n",
        "\n",
        "# Extract the sums of squares\n",
        "SST = anova_table['sum_sq']['groups']\n",
        "SSE = anova_table['sum_sq']['Residual']\n",
        "SSR = SST - SSE\n",
        "\n",
        "print(\"Total Sum of Squares (SST):\", SST)\n",
        "print(\"Explained Sum of Squares (SSE):\", SSE)\n",
        "print(\"Residual Sum of Squares (SSR):\", SSR)\n"
      ],
      "metadata": {
        "id": "Tiz5u_hKoRhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?**"
      ],
      "metadata": {
        "id": "Y5k9EXMfk8qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "# Define the data for the two factors\n",
        "factor_1 = [10, 12, 14, 15, 18, 13, 16, 19, 22, 15]\n",
        "factor_2 = [8, 9, 11, 13, 15, 7, 9, 12, 14, 13]\n",
        "response = [20, 22, 25, 28, 30, 21, 25, 27, 29, 24]\n",
        "\n",
        "# Create a dataframe with the data\n",
        "data = {'Factor_1': factor_1, 'Factor_2': factor_2, 'Response': response}\n",
        "\n",
        "# Fit the two-way ANOVA model\n",
        "model = ols('Response ~ Factor_1 + Factor_2 + Factor_1:Factor_2', data=data).fit()\n",
        "anova_table = sm.stats.anova_lm(model)\n",
        "\n",
        "# Extract the main effects and interaction effects\n",
        "main_effect_factor_1 = anova_table['sum_sq']['Factor_1']\n",
        "main_effect_factor_2 = anova_table['sum_sq']['Factor_2']\n",
        "interaction_effect = anova_table['sum_sq']['Factor_1:Factor_2']\n",
        "\n",
        "print(\"Main Effect of Factor 1:\", main_effect_factor_1)\n",
        "print(\"Main Effect of Factor 2:\", main_effect_factor_2)\n",
        "print(\"Interaction Effect:\", interaction_effect)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POk1Cwm1oUoY",
        "outputId": "dd825d72-0884-4c8c-dd87-f90ce6676895"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Main Effect of Factor 1: 76.28790035587203\n",
            "Main Effect of Factor 2: 16.381886094366987\n",
            "Interaction Effect: 1.0317489178306676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02. What can you conclude about the differences between the groups, and how would you interpret these results?**"
      ],
      "metadata": {
        "id": "G0F11fx8k8lB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When conducting a one-way ANOVA and obtaining an F-statistic of 5.23 and a p-value of 0.02, you can draw the following conclusions and interpret the results:\n",
        "\n",
        "1. Conclusions:\n",
        "   - There are statistically significant differences between the groups.\n",
        "   - The differences observed between the group means are unlikely to occur due to random chance alone.\n",
        "\n",
        "2. Interpretation:\n",
        "   - The F-statistic measures the ratio of variability between the groups to variability within the groups. In this case, an F-statistic of 5.23 suggests that the between-group variability is 5.23 times larger than the within-group variability.\n",
        "   - The p-value of 0.02 indicates the probability of obtaining an F-statistic as extreme as the observed one (or more extreme) if the null hypothesis is true (i.e., if the group means are equal). A p-value of 0.02 suggests that there is only a 2% chance of observing such a large F-statistic under the null hypothesis.\n",
        "   - Since the p-value is less than the chosen significance level (usually 0.05), we reject the null hypothesis. Therefore, we can conclude that there are significant differences between the groups.\n",
        "   - The significant differences imply that at least one group mean is different from the others. However, the ANOVA does not indicate which specific groups are different from each other. To determine the specific group differences, post hoc tests (e.g., Tukey's test, Bonferroni's test) can be conducted.\n",
        "\n",
        "In summary, with an F-statistic of 5.23 and a p-value of 0.02 in a one-way ANOVA, we conclude that there are statistically significant differences between the groups. The p-value indicates the likelihood of observing such differences by chance, and as it is less than the chosen significance level, we reject the null hypothesis. The ANOVA test provides evidence for group differences, but further post hoc tests are necessary to identify the specific group(s) that differ from each other."
      ],
      "metadata": {
        "id": "2ReckafDoXE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential consequences of using different methods to handle missing data?**"
      ],
      "metadata": {
        "id": "7hVFjLZQk8gQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a repeated measures ANOVA, missing data can pose challenges as the repeated measures nature requires complete data across all measurements for each participant. Handling missing data involves making decisions on how to handle the missing values in the analysis. Here are some common methods and their potential consequences:\n",
        "\n",
        "1. Complete Case Analysis (Listwise Deletion):\n",
        "   - This approach involves excluding any participant with missing data from the analysis.\n",
        "   - Potential consequences:\n",
        "     - Reduced sample size and loss of statistical power.\n",
        "     - Biased estimates if the missing data are not missing completely at random (MCAR).\n",
        "\n",
        "2. Pairwise Deletion:\n",
        "   - This approach uses available data for each pair of variables in the analysis, even if other variables have missing values.\n",
        "   - Potential consequences:\n",
        "     - Reduced sample size for specific comparisons.\n",
        "     - Different degrees of missingness may introduce bias in the estimates.\n",
        "     - Standard errors may be underestimated.\n",
        "\n",
        "3. Mean Imputation:\n",
        "   - Missing values are replaced with the mean of the available data for the respective variable.\n",
        "   - Potential consequences:\n",
        "     - Underestimation of variability and standard errors.\n",
        "     - Distorted relationships between variables.\n",
        "     - Artificially reduced standard errors and inflated statistical significance.\n",
        "\n",
        "4. Last Observation Carried Forward (LOCF):\n",
        "   - Missing values are replaced with the last observed value for the respective variable.\n",
        "   - Potential consequences:\n",
        "     - Potentially biased estimates if missingness is related to the underlying change in the variable over time.\n",
        "     - May not accurately capture the true values of the missing data.\n",
        "\n",
        "5. Multiple Imputation:\n",
        "   - Missing values are imputed multiple times based on the observed data, creating multiple complete datasets for analysis.\n",
        "   - Potential consequences:\n",
        "     - More accurate estimates compared to single imputation methods.\n",
        "     - Accounts for uncertainty associated with missing data.\n",
        "     - Increased complexity and computational demands.\n",
        "\n",
        "Each method for handling missing data has its advantages and disadvantages, and the choice should be based on the nature of the missing data, underlying assumptions, and research goals. It is important to consider potential biases and limitations introduced by each method, as different methods may yield different results and interpretations. Consultation with a statistician or expert in missing data methods is recommended to choose the most appropriate approach for handling missing data in a repeated measures ANOVA."
      ],
      "metadata": {
        "id": "czV5jvqnob70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide an example of a situation where a post-hoc test might be necessary.**"
      ],
      "metadata": {
        "id": "Ik1R_Kxck8bo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After conducting an ANOVA and finding significant differences among groups, post-hoc tests are used to determine specific pairwise differences between groups. Here are some common post-hoc tests and when to use each one:\n",
        "\n",
        "1. Tukey's Honestly Significant Difference (Tukey HSD):\n",
        "   - Use Tukey's HSD when you have three or more groups and want to compare all possible pairwise differences.\n",
        "   - Tukey's HSD controls the familywise error rate, ensuring that the overall Type I error rate across all comparisons is controlled at the desired significance level.\n",
        "   - It is a conservative test and suitable when the assumption of equal variances is met.\n",
        "\n",
        "2. Bonferroni Correction:\n",
        "   - Bonferroni correction is used to adjust the significance level for multiple pairwise comparisons.\n",
        "   - Divide the desired significance level (e.g., 0.05) by the number of pairwise comparisons to obtain a more stringent significance level for each comparison.\n",
        "   - Bonferroni correction is suitable when you have a small number of pairwise comparisons.\n",
        "\n",
        "3. Dunnett's Test:\n",
        "   - Use Dunnett's test when you have one control group and want to compare other groups to the control group.\n",
        "   - It controls the Type I error rate for multiple comparisons by comparing each group to a control group while accounting for the correlation among the comparisons.\n",
        "   - Dunnett's test is particularly useful when there is a single control group and interest lies in comparing other groups to that control.\n",
        "\n",
        "4. Scheffé's Test:\n",
        "   - Scheffé's test is a conservative post-hoc test that allows for all possible comparisons among groups, even if they are not pre-planned.\n",
        "   - It is suitable when there are a large number of comparisons or when you are interested in examining specific contrasts not covered by other post-hoc tests.\n",
        "   - Scheffé's test provides wider confidence intervals and tends to be less powerful compared to other post-hoc tests.\n",
        "\n",
        "Example:\n",
        "Suppose you conducted an ANOVA to compare the mean scores of three different teaching methods (Method A, Method B, Method C) in terms of student performance. The ANOVA results indicate a significant difference among the groups. To determine which specific teaching methods differ from each other, you would conduct a post-hoc test.\n",
        "\n",
        "For instance, you could use Tukey's HSD to compare all possible pairwise differences between the teaching methods. This test would help identify which pairs of teaching methods have significantly different mean scores.\n",
        "\n",
        "Post-hoc tests are necessary when ANOVA results indicate significant differences among groups, but they do not specify the specific pairwise differences. These tests enable a more detailed analysis, allowing you to make specific comparisons and determine which groups significantly differ from each other."
      ],
      "metadata": {
        "id": "CPsUs2i_ofT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from 50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python to determine if there are any significant differences between the mean weight loss of the three diets.\n",
        "Report the F-statistic and p-value, and interpret the results.**"
      ],
      "metadata": {
        "id": "nkKNwMjOk8XX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Define the weight loss data for each diet\n",
        "diet_A = [2.5, 3.0, 2.8, 2.6, 3.2, 2.9, 3.1, 2.7, 2.8, 2.4, 2.6, 2.9, 3.3, 2.7, 2.8, 2.5, 3.0, 2.9, 2.7, 2.6,\n",
        "          2.8, 2.7, 2.6, 2.9, 3.2, 2.8, 2.9, 2.7, 2.6, 2.8, 3.0, 2.9, 2.7, 2.6, 3.0, 2.8, 2.7, 2.6, 2.9, 2.7,\n",
        "          2.8, 2.5, 3.0, 2.9, 2.7, 2.6, 2.8, 2.7, 2.9, 2.6]\n",
        "\n",
        "diet_B = [2.3, 2.1, 2.4, 2.2, 2.5, 2.3, 2.4, 2.1, 2.2, 2.0, 2.3, 2.2, 2.4, 2.3, 2.1, 2.2, 2.5, 2.3, 2.4, 2.1,\n",
        "          2.2, 2.5, 2.3, 2.1, 2.4, 2.2, 2.3, 2.1, 2.2, 2.5, 2.3, 2.4, 2.1, 2.2, 2.3, 2.1, 2.4, 2.2, 2.3, 2.1,\n",
        "          2.2, 2.5, 2.3, 2.4, 2.1, 2.2, 2.3, 2.1, 2.4, 2.2]\n",
        "\n",
        "diet_C = [1.8, 2.0, 1.9, 1.7, 1.8, 1.9, 1.6, 1.7, 1.9, 1.8, 1.9, 1.7, 1.8, 1.9, 1.7, 1.8, 1.9, 1.6, 1.7, 1.9,\n",
        "          1.8, 1.9, 1.7, 1.8, 1.9, 1.7, 1.8, 1.9, 1.6, 1.7, 1.9, 1.8, 1.9, 1.7, 1.8, 1.9, 1.7, 1.8, 1.9, 1.6,\n",
        "          1.7, 1.9, 1.8, 1.9, 1.7, 1.8, 1.9, 1.7, 1.8, 1.9]\n",
        "\n",
        "# Perform the one-way ANOVA\n",
        "f_statistic, p_value = f_oneway(diet_A, diet_B, diet_C)\n",
        "\n",
        "# Print the results\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"p-value:\", p_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSeOpDkxojaa",
        "outputId": "8c9c20cf-32e6-4bdf-9e34-9fe270958bbb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 560.3189135434154\n",
            "p-value: 1.6891865516721207e-69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10. A company wants to know if there are any significant differences in the average time it takes to complete a task using three different software programs: Program A, Program B, and Program C. They randomly assign 30 employees to one of the programs and record the time it takes each employee to\n",
        "complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or interaction effects between the software programs and employee experience level (novice vs. experienced). Report the F-statistics and p-values, and interpret the results.**"
      ],
      "metadata": {
        "id": "ZI96UWNnk8Sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "# Define the data\n",
        "data = {\n",
        "    'Time': [15, 18, 16, 20, 22, 17, 19, 21, 14, 15, 16, 19, 18, 17, 20, 21, 16, 19, 20, 22, 23, 17, 16, 18, 21, 20, 19, 18, 19, 17],\n",
        "    'Program': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'A', 'A', 'A'],\n",
        "    'Experience': ['Novice', 'Experienced', 'Novice', 'Experienced', 'Novice', 'Experienced', 'Novice', 'Experienced', 'Novice', 'Experienced', 'Novice', 'Experienced', 'Novice', 'Experienced', 'Novice', 'Experienced', 'Novice', 'Experienced', 'Novice', 'Experienced', 'Novice', 'Experienced', 'Novice', 'Experienced', 'Novice', 'Experienced', 'Novice', 'Experienced', 'Novice', 'Experienced']\n",
        "}\n",
        "\n",
        "# Create a dataframe\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Fit the two-way ANOVA model\n",
        "model = ols('Time ~ Program + Experience + Program:Experience', data=df).fit()\n",
        "anova_table = sm.stats.anova_lm(model)\n",
        "\n",
        "# Extract the F-statistics and p-values\n",
        "f_statistic_program = anova_table['F']['Program']\n",
        "p_value_program = anova_table['PR(>F)']['Program']\n",
        "f_statistic_experience = anova_table['F']['Experience']\n",
        "p_value_experience = anova_table['PR(>F)']['Experience']\n",
        "f_statistic_interaction = anova_table['F']['Program:Experience']\n",
        "p_value_interaction = anova_table['PR(>F)']['Program:Experience']\n",
        "\n",
        "# Print the results\n",
        "print(\"Main Effect of Program:\")\n",
        "print(\"F-statistic:\", f_statistic_program)\n",
        "print(\"p-value:\", p_value_program)\n",
        "print()\n",
        "print(\"Main Effect of Experience:\")\n",
        "print(\"F-statistic:\", f_statistic_experience)\n",
        "print(\"p-value:\", p_value_experience)\n",
        "print()\n",
        "print(\"Interaction Effect:\")\n",
        "print(\"F-statistic:\", f_statistic_interaction)\n",
        "print(\"p-value:\", p_value_interaction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQRn7j4FpRv6",
        "outputId": "bdfac136-1449-447c-b01a-035760089941"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Main Effect of Program:\n",
            "F-statistic: 0.2517099863201095\n",
            "p-value: 0.7794970163244002\n",
            "\n",
            "Main Effect of Experience:\n",
            "F-statistic: 0.1856158827798553\n",
            "p-value: 0.6704355470839992\n",
            "\n",
            "Interaction Effect:\n",
            "F-statistic: 1.3881023931455791\n",
            "p-value: 0.2688726638814166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11. An educational researcher is interested in whether a new teaching method improves student test scores. They randomly assign 100 students to either the control group (traditional teaching method) or the experimental group (new teaching method) and administer a test at the end of the semester. Conduct a\n",
        "two-sample t-test using Python to determine if there are any significant differences in test scores between the two groups. If the results are significant, follow up with a post-hoc test to determine which\n",
        "group(s) differ significantly from each other.**"
      ],
      "metadata": {
        "id": "oyC2i9Mrk8OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Define the test scores for the control group and experimental group\n",
        "control_scores = [78, 80, 75, 82, 85, 76, 79, 83, 77, 81, 84, 78, 82, 80, 79, 77, 85, 79, 83, 80, 81, 78, 77, 79, 82, 83, 80, 81, 76, 78]\n",
        "experimental_scores = [80, 84, 82, 86, 87, 81, 83, 85, 82, 84, 83, 87, 81, 79, 82, 86, 83, 80, 82, 85, 82, 86, 84, 83, 80, 81, 84, 85, 82, 84]\n",
        "\n",
        "# Perform the two-sample t-test\n",
        "t_statistic, p_value = ttest_ind(control_scores, experimental_scores)\n",
        "\n",
        "# Print the results\n",
        "print(\"Two-Sample T-Test:\")\n",
        "print(\"t-statistic:\", t_statistic)\n",
        "print(\"p-value:\", p_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISslAaRJpkod",
        "outputId": "c2066243-6bec-4925-9c87-a60a9489b94d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Two-Sample T-Test:\n",
            "t-statistic: -5.0028680690694936\n",
            "p-value: 5.5603752641223795e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we have the time data for each employee completing the task, along with the software program used (Program) and the employee's experience level (Experience). We create a dataframe df with the data.\n",
        "\n",
        "We fit a two-way ANOVA model using the ols function from statsmodels and specify the model formula, including the main effects of Program and Experience, as well as their interaction effect (Program:Experience).\n",
        "\n",
        "After fitting the model, we extract the F-statistics and p-values from the resulting ANOVA table for the main effects and interaction effect.\n",
        "\n",
        "Interpreting the results:\n",
        "\n",
        "Main Effect of Program: If the p-value associated with the Program factor is below the chosen significance level (e.g., 0.05), we conclude that there is a significant main effect of the software program on the task completion time. The F-statistic measures the ratio of the between-group variability to the within-group variability for the Program factor.\n",
        "Main Effect of Experience: If the p-value associated with the Experience factor is below the chosen significance level, we conclude that there is a significant main effect of employee experience level on the task completion time. The F-statistic measures the ratio of the between-group variability to the within-group variability for the Experience factor.\n",
        "Interaction Effect: If the p-value associated with the interaction term (Program:Experience) is below the chosen significance level, we conclude that there is a significant interaction effect between the software program and employee experience level. The F-statistic measures the significance of the interaction effect, indicating if the impact of the software program on task completion time differs based on employee experience level.\n",
        "Make sure to adjust the data and column names (Time, Program, Experience) accordingly if your data has different values or column labels."
      ],
      "metadata": {
        "id": "y3bwU8X3poxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q12. A researcher wants to know if there are any significant differences in the average daily sales of three retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store on those days. Conduct a repeated measures ANOVA using Python to determine if there are any significant differences in sales between the three stores. If the results are significant, follow up with a post- hoc test to determine which store(s) differ significantly from each other.**"
      ],
      "metadata": {
        "id": "T2zMKxXHm81t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To conduct a repeated measures ANOVA and perform a post-hoc test in Python, you can use the `statsmodels` library. Here's how you can analyze the data and determine if there are significant differences in sales between the three stores:\n",
        "\n",
        "First, let's assume you have the sales data for Store A, Store B, and Store C in three separate lists or arrays: `store_a_sales`, `store_b_sales`, and `store_c_sales`, each containing 30 daily sales values.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# Combine the sales data into a single pandas DataFrame\n",
        "sales_data = pd.DataFrame({'Store A': store_a_sales,\n",
        "                           'Store B': store_b_sales,\n",
        "                           'Store C': store_c_sales})\n",
        "\n",
        "# Reshape the data for repeated measures ANOVA\n",
        "sales_data = pd.melt(sales_data, value_name='Sales', var_name='Store')\n",
        "\n",
        "# Create a formula for the ANOVA model\n",
        "formula = 'Sales ~ C(Store)'\n",
        "\n",
        "# Fit the repeated measures ANOVA model\n",
        "model = ols(formula, sales_data).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "# Print the ANOVA results\n",
        "print(anova_table)\n",
        "\n",
        "# Perform post-hoc test (pairwise comparisons) if ANOVA results are significant\n",
        "if anova_table['PR(>F)'][0] < 0.05:\n",
        "    posthoc = pairwise_tukeyhsd(sales_data['Sales'], sales_data['Store'])\n",
        "    print(posthoc)\n",
        "```\n",
        "\n",
        "In the above code, we start by combining the sales data into a pandas DataFrame, where each store's sales are stored in separate columns. Then, we reshape the data using `pd.melt()` to create a \"long\" format suitable for repeated measures ANOVA.\n",
        "\n",
        "Next, we define the formula for the ANOVA model (`formula = 'Sales ~ C(Store)'`) and fit the model using `ols()` from `statsmodels.formula.api`. The `typ=2` argument in `sm.stats.anova_lm()` specifies that we want to calculate the Type 2 sum of squares.\n",
        "\n",
        "After fitting the model, we print the ANOVA table using `anova_lm()` from `statsmodels.stats.anova_lm()`.\n",
        "\n",
        "If the p-value (PR(>F)) in the ANOVA table is less than 0.05 (or any chosen significance level), we consider the results to be significant. In that case, we can perform a post-hoc test to determine which store(s) differ significantly from each other.\n",
        "\n",
        "The code includes the pairwise Tukey's HSD test (`pairwise_tukeyhsd()`) from `statsmodels.stats.multicomp` to conduct the post-hoc test. It compares all possible pairs of stores and provides statistical comparisons and confidence intervals.\n",
        "\n",
        "Please note that you need to have the `statsmodels` library installed to run this code. You can install it using `pip install statsmodels`."
      ],
      "metadata": {
        "id": "jOf1hSnrq9Cw"
      }
    }
  ]
}